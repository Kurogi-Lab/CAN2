<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>Execution examples</title>
</head>
<meta charset="UTF-8">
<!--<meta http-equiv="Content-Style-Type" content="text/css">-->
<!-- ------------------------------- -->
<!--from https://www.conifer.jp/csstest/css-list-marker-paren.html-->
<style>
ol.paren li  {
        list-style-type:none;
        counter-increment: cnt;
        text-indent:-2.5em;
        padding:0;
        }
    ol.paren0 li  {
        list-style-type:none;
        text-indent:-3em;
        padding:0;
        }
    ol.paren li:before  {
        display: marker;
        content: "["counter(cnt) "] ";
}
</style>
<!-- ------------------------------- -->
</head>

<body>
<h1>Execution examples of CAN2</h1>
Execution examples of regression and time-series IOS prediction by the single and bagging CAN2s are shown below.
See <a href="#ref1">[1]</a>,
<a href="#ref2">[2]</a> and
<a href="#ref3">[3]</a> for the details of regression, bagging, IOS prediction by CAN2, respectively.

<ol>
  <li> Set the root directory: <br>
      <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
    export d0=$PWD;echo $d0 #set the root directory involving data, can2py, can2comp, etc.

</div>
  <li> Data preparation: <br>
      <ul>
	<li> Regression data (<a href="#Fig.1">Fig.1</a>) : made by the following steps<br>
	    <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
	    cd ${d0}/can2py<br>
	    export fn=Geo1d  ntrain=100 restest=50 extest=10 k=1;<br>
	    python makesamples.py -msd $fn,$ntrain,$restest,$extest <br>
	    dst=${d0}/data/${fn}_${ntrain}_${restest}_${extest}<br>
	    mkdir -p $dst<br>
	    cp tmp/train.csv tmp/test.csv $dst/<br>
	    </div>
	<li> Time-series data (<a href="#Fig.2">Fig.2</a>): see <a href="#ref3">[3]</a> for data creation via using GMP: <br>
	    ${d0}/data/lorenz1e-8T0.025n10000p256m1_gmp.txt #Only the first column is used below<br>
	    <img class="fit-picture" src="Geo1d_100_50_10.png" alt="Fig.1" height="200" width="300" > <a name="Fig.1">Fig.1</a>
	    <img class="fit-picture" src="lorenz1e-8T0.025n10000p256m1_gmp.png" alt="Fig.2" height="200" width="300" > <a name="Fig.2">Fig.2</a>
</ul>
  <li> C program
      <ul>
	<li> Regression
	    <ol>
	      <li> common parameter setting<br>
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  cd $d0/can2comp;make<br>
		  export fn=Geo1d ntrain=100 restest=50 extest=10 k=1;<br>
		  export dst=${d0}/data/${fn}_${ntrain}_${restest}_${extest}<br>
		  export fntrain=$dst/train.csv fntest=$dst/test.csv fnpred=predict+.dat<br>
		  </div>
	      <li> single CAN2 
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  make data-clean;<br>
		  export N=30 seed=0 k=1 T=100;<br>
		  ensrs $fntrain -1:$fntest $N k:$k T:$T BIAS:1 Lstd:0:2 ib:0:0:0:0 vm2:-1 seed:$seed Tpinv:-1 nop:1 DISP:0<br>
		  #(or) ensrs $fntrain 2:1:1:1 $N bg:$fntest k:$k T:$T BIAS:1 Lstd:0:2 ib:0:0:0:0 vm2:-1 seed:$seed Tpinv:-1 nop:1 
		  export fntest=$fntest fnpred=./result-ensrs/tmp/train+test+s${s}0j0k${k}N${N}pred.dat;../sh/show${k}dpred.sh<br>
		  </div>
		  #Results (see <a href="#Fig.3">Fig.3</a>) (smallest MSEtst=1.161e-05 (MSE for the test data) is achieved with N=30)<br>
		  #100(0.030s) 7.274e-05 5.076e-04 #ep(time),MSEtr,MSEtst k1 n100:71 N20 s0<br>
		  #100(0.033s) 6.660e-06 1.161e-05 #ep(time),MSEtr,MSEtst k1 n100:71 N30 s0 ***<br>
		  #100(0.031s) 1.563e-05 3.368e-04 #ep(time),MSEtr,MSEtst k1 n100:71 N40 s0<br>
	      <li> Bagging CAN2 
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  make data-clean;<br>
		  export N=50 a=2.2 b=100 seed=0 k=1 T=100;<br>
		  ensrs $fntrain 2:$b:$a:1 $N bg:$fntest k:$k T:$T BIAS:1 Lstd:0:2 ib:0:0:0:0 vm2:-1 seed:$seed Tpinv:-1 nop:1<br>
		  export fntest=$fntest fnpred=predict+.dat;../sh/show${k}dpred.sh
		  </div>
		  #Results (see <a href="#Fig.4">Fig.4</a>)<font color="#ff000"> (stably small MSE w.r.t. change of N)</font><br>
		  #[100,-1](1.5s) 1.706e-05 #[T,Tpinv](time) n220,71 k1 N40 b100 a2.2 s0 nop1 m_cpu6<br>
		  #[100,-1](1.6s) 1.186e-05 #[T,Tpinv](time) n220,71 k1 N50 b100 a2.2 s0 nop1 m_cpu6***<br>
		  #[100,-1](1.8s) 1.616e-05 #[T,Tpinv](time) n220,71 k1 N60 b100 a2.2 s0 nop1 m_cpu6<br>
	      <li> OOB(out-of-bag) estimate (see <a href="#ref2">[2]</a>):To obtain the above parameters a=2.2 and N=40  without test data
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  export fntrains=$dst/trains.csv<br>
		  sort -g -k 1 $fntrain > $fntrains  #To display the result shown below<br>
make data-clean<br>
export N=50 a=2.2 b=300 seed=1 k=1 T=100; #Obtain optimal a and N by trial and ePCrror<br>
		  ensrs $fntrains 2:$b:$a:$seed $N bg:/dev/null k:$k T:$T BIAS:1 Lstd:0:2 ib:0:0:0:0 vm2:-1 Tpinv:-1 nop:1 <br>
		  export fntest=$fntrains fnpred=predict+oob.dat;../sh/show${k}dpred.sh #Display the result<br>
		  </div>
		  #Results of oob estimate for fntrain=$d0/data/Geo1d_100_50_10/train.csv<br>
  #a=2.2 (Do not care about "inf H-nan" below<br>
#Ltst(n10.6*100)2.443e-05 Lib(n89.4*100)inf H-nan, nTest10.6=11.1=exp(-2.20)*nGiven100method:2:100:2.2:1N30.<br>
		  #Ltst(n10.6*100)2.287e-05 Lib(n89.4*100)inf H-nan, nTest10.6=11.1=exp(-2.20)*nGiven100method:2:100:2.2:1N40.***(2.287e-05)<br>
		  #Ltst(n32.0*100)2.413e-05 Lib(n267.9*100)1.660e-01 H7.086e-01, nTest10.7=11.1=exp(-2.20)*nGiven100method:2:300:2.2:1N50.<br>
#a=2.3<br>
#Ltst(n9.7*100)6.488e-05 Lib(n90.3*100)inf H-nan, nTest9.7=10.0=exp(-2.30)*nGiven100method:2:100:2.3:0N40.<br>
#Ltst(n9.7*100)4.635e-05 Lib(n90.3*100)inf H-nan, nTest9.7=10.0=exp(-2.30)*nGiven100method:2:100:2.3:0N50.<br>
#Ltst(n9.7*100)5.402e-05 Lib(n90.3*100)inf H-nan, nTest9.7=10.0=exp(-2.30)*nGiven100method:2:100:2.3:0N60.<br>
#a=2.1<br>
#Ltst(n11.9*100)4.721e-05 Lib(n88.1*100)inf H-nan, nTest11.9=12.2=exp(-2.10)*nGiven100method:2:100:2.1:1N40.<br>
#Ltst(n11.9*100)3.921e-05 Lib(n88.1*100)inf H-nan, nTest11.9=12.2=exp(-2.10)*nGiven100method:2:100:2.1:1N50.<br>
		  #Ltst(n11.9*100)5.038e-05 Lib(n88.1*100)inf H-nan, nTest11.9=12.2=exp(-2.10)*nGiven100method:2:100:2.1:1N60.<br>
#####<br>
#Additional results of oob estimate for fntrain=$d0/data/Geo1d_1000_50_10/train.csv<br>
#Ltst(n222.2*1000)2.805e-07 Lib(n77.8*1000)2.920e-01 H8.634e-01, nTest740.7=740.8=exp(-0.30)*nGiven1000method:2:300:0.3:1N50.<br>
#Ltst(n149.0*1000)9.212e-08 Lib(n151.0*1000)2.920e-01 H8.657e-01, nTest496.5=496.6=exp(-0.70)*nGiven1000method:2:300:0.7:1N50. 2-fold CV<br>
#Ltst(n110.4*1000)8.835e-08 Lib(n189.6*1000)2.920e-01 H8.660e-01, nTest368.1=367.9=exp(-1.00)*nGiven1000method:2:300:1.0:1N50. .632 bootstrap<br>
#Ltst(n60.7*1000)8.224e-08 Lib(n239.3*1000)2.920e-01 H8.662e-01, nTest202.2=201.9=exp(-1.60)*nGiven1000method:2:300:1.6:1N50.*** 5-fold CV<br>
#Ltst(n30.1*1000)1.020e-07 Lib(n269.9*1000)2.920e-01 H8.663e-01, nTest100.2=100.3=exp(-2.30)*nGiven1000method:2:300:2.3:1N50. 10-fold CV<br>
#####<br>
#Additional results of oob estimate for $d0/data/Geo1d_5000_50_10/train.csv<br>
#Ltst(n222.2*5000)2.354e-08 Lib(n77.8*5000)2.831e-01 H8.853e-01, nTest3703.9=3704.1=exp(-0.30)*nGiven5000method:2:300:0.3:1N50.<br>
#Ltst(n149.0*5000)2.300e-08 Lib(n151.0*5000)2.831e-01 H8.872e-01, nTest2482.6=2482.9=exp(-0.70)*nGiven5000method:2:300:0.7:1N50. 2-fold CV<br>
#Ltst(n110.3*5000)2.256e-08 Lib(n189.7*5000)2.831e-01 H8.873e-01, nTest1838.7=1839.4=exp(-1.00)*nGiven5000method:2:300:1.0:1N50.***a=1.0 oob best for ntest5000<br>
#Ltst(n60.6*5000)2.667e-08 Lib(n239.4*5000)2.831e-01 H8.877e-01, nTest1010.6=1009.5=exp(-1.60)*nGiven5000method:2:300:1.6:1N50. 5-fold CV<br>
#Ltst(n30.2*5000)3.960e-08 Lib(n269.8*5000)2.831e-01 H8.878e-01, nTest503.5=501.3=exp(-2.30)*nGiven5000method:2:300:2.3:1N50.  10-fold CV<br>

	    </ol>
	    <img class="fit-picture" src="regression-singleCAN2N30s0.jpg" alt="Fig.3" height="200" width="300" > <a name="Fig.3">Fig.3</a>
	    <img class="fit-picture" src="regression-baggingCAN2N50a2.2s0.jpg" alt="Fig.3" height="200" width="300" > <a name="Fig.4">Fig.4</a>
	<li> Time-series IOS prediction (IOS:iterated one-step ahead prediction)
	    <ol>
	      <li> common parameter setting<br>
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  cd $d0/can2comp;make<br>
		  export fn=$d0/data/lorenz1e-8T0.025n10000p256m1_gmp.txt<br>
</div>
	      <li> single CAN2 
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  export T=100 Tpinv=-1 k=8 N=50 seed=1 tp0=2000 tpD=1 Ey=15 a=1 b=1 nop=1 n_compare=6 v_thresh=0.2 vmin=3 vmin2=0 v_ratio=0.5 width=0.2 l_mode=1 gamma=1.4e-4 nentropy_thresh=0.7 n_display=5 rot_x=50 rot_y=350 y=-18.5:18.5:0:1 <br>
		  make data-clean;<br>
		  ensrs $fn 2:${b}:${a}:${seed} $N-$N:1 t:0-2000:$tp0-$(($tp0+500)):$tpD:$Ey bg:/dev/null ib:0:0:0:0 k:$k g:$gamma w:$width T:$T vt:$v_thresh vr:$v_ratio lossall:1 DISP:2 y:$y x:$y nop:1 Tpinv:-1<br>
		  #To see the result saved in tmp/pred2000-2500.dat, do <br>
		  cat &gt tmp/y.plt &lt&ltEOF <br>
		  set grid;set term png;set output "tmp/y.png";<br>
		  plot "tmp/pred2000-2500.dat" using 2:1 w l t "yp", "" using 2:3 w l t "y", "" using 2:(\$1-\$3) w l t "err=yp-y"<br>
		  EOF<br>
		  gnuplot tmp/y.plt;eog tmp/y.png<br>
		  </div>
		  #Results (see <a href="#Fig.5">Fig.5</a>)(H:predictable horison with the error threshold Ey=15)<br>
		  #[100,-1](0.2s) #[T,Tinv] k8 N40 b1(nens1) a1 seed1 nop1 m_cpu6 0-2000:2000-2500:15H126<br>
		  #[100,-1](0.3s) #[T,Tinv] k8 N50 b1(nens1) a1 seed1 nop1 m_cpu6 0-2000:2000-2500:15H153***<br>
		  #[100,-1](0.3s) #[T,Tinv] k8 N60 b1(nens1) a1 seed1 nop1 m_cpu6 0-2000:2000-2500:15H24<br>
	      <li> Bagging CAN2 <br>
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  export T=100 Tpinv=-1 k=8 N=50 seed=1 tp0=2000 tpD=1 Ey=15 a=0.7 b=20 nop=1 n_compare=6 v_thresh=0.2 vmin=3 vmin2=0 v_ratio=0.5 width=0.2 l_mode=1 gamma=1.4e-4 nentropy_thresh=0.7 n_display=5 rot_x=50 rot_y=350 y=-18.5:18.5:0:1 <br>
		  make data-clean;<br>
		  ensrs $fn 2:${b}:${a}:${seed} $N-$N:1 t:0-2000:$tp0-$(($tp0+500)):$tpD:$Ey bg:/dev/null ib:0:0:0:0 k:$k g:$gamma w:$width T:$T vt:$v_thresh vr:$v_ratio lossall:1 DISP:2 y:$y x:$y nop:1 Tpinv:-1<br>
		  </div>
		  #Results (see <a href="#Fig.6">Fig.6</a>; <font color="#ff000">longest predictable horizon H=224 is achieved with N=60</font>; a method to obtain a good parameter N is proposed in <a href="#ref3">[3]</a><br>
		  #[100,-1](0.9s) #[T,Tinv] k8 N40 b20(nens1) a0.7 seed1 nop1 m_cpu6 0-2000:2000-2500:15H157<br>
		  #[100,-1](1.0s) #[T,Tinv] k8 N50 b20(nens1) a0.7 seed1 nop1 m_cpu6 0-2000:2000-2500:15H224 ***<br>
		  #[100,-1](1.3s) #[T,Tinv] k8 N60 b20(nens1) a0.7 seed1 nop1 m_cpu6 0-2000:2000-2500:15H199<br>
	    </ol>
	    <img class="fit-picture" src="IOSpred-singleCAN2cN50s1-y.png" alt="Fig.5" height="200" width="300" > <a name="Fig.5">Fig.5</a>
	    <img class="fit-picture" src="IOSpred-baggingCAN2cN50a0.7b20s1-y.png" alt="Fig.6" height="200" width="300" > <a name="Fig.6">Fig.6</a>
      </ul>
  <li> Python program
      <ul>
	<li> Regression<br>
	    <ol>
	      <li> single CAN2<br>
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  cd ${d0}/can2py<br>
		  export fn=Geo1d  ntrain=100 restest=50 extest=10 k=1;<br>
		  export dst=${d0}/data/${fn}_${ntrain}_${restest}_${extest}
		  export fntrain=$dst/train.csv fntest=$dst/test.csv fnpred=$dst/pred.csv<br>
		  export T=100 N=100 k=1;<br>
		  make data-clean<br>
		  python can2.py -fn $fntrain,$fntest,$fnpred -k $k -in $N,6,0.2,3,0,0.5,0.2 -ex 1,0.05,2.2,$T,5,50,350 --gpu -1 -DISP 1 -nop 1<br>
		  ../sh/show${k}dpred.sh
		  </div>
		  #Results (see <a href="#Fig.7">Fig.7</a>,<a href="#Fig.8">Fig.8</a>) (smallest MSE=2.627e-05 (for the test data) is achieved with N=100)<br>
		  #100(1.050s) 7.310e-06 4.198e-05 #ep(time),MSEtr,MSE n100,71 k1 N90 T100,1000 seed0 nop1<br>
		  #100(1.228s) 5.843e-06 2.627e-05 #ep(time),MSEtr,MSE n100,71 k1 N100 T100,1000 seed0 nop1<br>
		  #100(1.249s) 5.843e-06 2.627e-05 #ep(time),MSEtr,MSE n100,71 k1 N110 T100,1000 seed0 nop1<br>
	      <li> Bagging CAN2
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  cd ${d0}/can2py<br>
		  export T=100 N=50 k=1 Tpinv=-1 seed=1 m_cpu=0 b=100 a=2.2 nop=1<br>
		  make data-clean<br>
		  python ensrs.py -fn $fntrain,$fntest,$fnpred -k $k,0 -in $N,6,0.2,3,0,0.5,0.2 -ex 1,0.05,0.7,$T,5,50,350 -DISP 0 -Tpinv $Tpinv -s $seed -nop $nop -bag $b,$a,$seed,$m_cpu<br>
		  ../sh/show${k}dpred.sh
		  </div>
		  #Results (see <a href="#Fig.9">Fig.9</a>)<br>
		  #[100,-1](53.1s) 3.052e-05 #[T,Tpinv] MSE n100,71 k1 N40 b100 a2.2 s1 m6 seed1 nop1<br>
		  #[100,-1](55.5s) 2.105e-05 #[T,Tpinv] MSE n100,71 k1 N50 b100 a2.2 s1 m6 seed1 nop1***<br>
		  #[100,-1](74.9s) 2.275e-05 #[T,Tpinv] MSE n100,71 k1 N60 b100 a2.2 s1 m6 seed1 nop1<br>
		  <br>
	    </ol>
	    <img class="fit-picture" src="regression-singleCAN2pyN100s0-mse.jpg" alt="Fig.7" height="150" width="300" > <a name="Fig.7">Fig.7</a>
	    <img class="fit-picture" src="regression-singleCAN2pyN100s0.jpg" alt="Fig.8" height="200" width="300" > <a name="Fig.8">Fig.8</a>
	    <img class="fit-picture" src="regression-baggingCAN2pyN60a2.2b100s1.jpg" alt="Fig.7" height="200" width="300" > <a name="Fig.9">Fig.9</a>
	<li> Time-series IOS prediction
	    <ol>
	      <li> Single CAN2 (try the following command with different seed=0,1,2,...)
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  export T=100 Tpinv=-1 k=10 N=50 seed=2 tp0=2000 tpD=1 Ey=15 nop=1 n_compare=6 v_thresh=0.2 vmin=3 vmin2=0 v_ratio=0.5 width=0.2 l_mode=1 gamma0=0.05 nentropy_thresh=0.7 n_display=5 rot_x=50 rot_y=350 y=-18.5,18.5,0,1<br>
		  export fn=$d0/data/lorenz1e-8T0.025n10000p256m1_gmp.txt<br>
		  export fns=$d0/data/lorenz1e-8T0.025n10000p256m1_gmp+null+N50k10s${seed}.net<br>
		  make data-clean<br>
		  python can2.py -fn $fn -k $k -t 0-2000:$tp0-$(($tp0+500)):$tpD:$Ey -in $N,$n_compare,$v_thresh,$vmin,$vmin2,$v_ratio,$width -ex $l_mode,$gamma0,$nentropy_thresh,$T,$n_display,$rot_x,$rot_y --gpu -1 -DISP 1 -Tpinv $Tpinv -s $seed -nop $nop -y " $y" -fns $fns<br>
		  </div>
		  #Results (see <a href="#Fig.10">Fig.10</a>,<a href="#Fig.11">Fig.11</a>)<br>
		  #100(16.821s) 4.635e-05 1.238e-04 #ep(time),MSEtr,MSE n1990,500 k10 N50 T100,-1 seed0 nop1 t0-2000:2000-2500:1:15H22 predTime0.018s<br>
		  #100(16.885s) 4.113e-05 1.174e-04 #ep(time),MSEtr,MSE n1990,500 k10 N50 T100,-1 seed1 nop1 t0-2000:2000-2500:1:15H194 predTime0.018s<br>
		  #100(17.034s) 4.211e-05 1.293e-04 #ep(time),MSEtr,MSE n1990,500 k10 N50 T100,-1 seed2 nop1 t0-2000:2000-2500:1:15H101 predTime0.017s<br>
#    ...<br>
	      <li> Bagging CAN2<br>
		  <div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333; background-color: #ffff99;">
		  export T=100 Tpinv=-1 k=10 N=50 seed=10 b=20 a=1.0 tp0=2000 Ey=15 nop=1 n_compare=6 v_thresh=0.2 vmin=3 vmin2=0 v_ratio=0.5 width=0.2 l_mode=1 gamma0=0.05 nentropy_thresh=0.7 n_display=5 rot_x=50 rot_y=350 y=-18.5,18.5,0,1 m_cpu=0 fn=$d0/data/lorenz1e-8T0.025n10000p256m1_gmp.txt <br>
		  make data-clean<br>
		  python ensrs.py -fn $fn,,tmp/msp${tp0}.dat -k $k -t 0-2000:$tp0-$(($tp0+500)):1:$Ey -in $N,$n_compare,$v_thresh,$vmin,$vmin2,$v_ratio,$width -ex $l_mode,$gamma0,$nentropy_thresh,$T,$n_display,$rot_x,$rot_y --gpu -1 -DISP 0 -Tpinv $Tpinv -nop $nop -y " $y" -bag $b,$a,$seed,$m_cpu
		  </div>
		  #Results (see <a href="#Fig.12">Fig.12</a>)<br>
		  #[100,-1](226.9s) #[T,Tpinv] k10 N50 b20a1.0s10m6 nop1 t0-2000:2000-2500:1:15H159 seed0<br>
		  #[100,-1](227.1s) #[T,Tpinv] k10 N50 b20a1.0s0m6 nop1 t0-2000:2000-2500:1:15H158 seed1<br>
	    </ol>
	    <img class="fit-picture" src="IOSpred-singleCAN2pyN50s2-mse.jpg" alt="Fig.10" height="200" width="300" > <a name="Fig.10">Fig.10</a>
	    <img class="fit-picture" src="IOSpred-singleCAN2pyN50s2-y.jpg" alt="Fig.11" height="200" width="300" > <a name="Fig.11">Fig.11</a>
	    <img class="fit-picture" src="IOSpred-baggingCAN2pyN50a1b20s10-y.jpg" alt="Fig.12" height="200" width="300" > <a name="Fig.12">Fig.12</a>
	    
	    
      </ul>
  <li> Notes
      <ul>
	<li> Results of C and Python are different owing mainly to
	    slightly different algorithms and parameters optimized for each language.
	<li> Execution time of C is smaller than Python.
	<li> Computational cost of the CAN2 is lower than many 
	    other machine learning methods 
	    mainly because it uses the learning scheme consisting of
	    iterated steps of
	    (a) local gradient descent optimization of two weight vectors near the input vector,
	    (b) linear least squre optimization of a selected associative matrix,
	    (c) reinitialization based on asymptotic optimality of error distortions
	    avoiding local minimum (see <a href="#ref1">[1]</a>).
	<li> The CAN2 has advantages on (A) prediction accuracy in learning
	    the functions with piecewise different smoothness (see <a href="#ref1">[1]</a>),
	    (B) ability of extrapolation as well as interpolation (see <a href="#ref1">[1]</a>),
	    (C) comutational cost, (D) extraction of piecewise linear predictive coefficients
	    in the applications such as speech and speaker recognition (audio processing) and control
	    (see <a href="#ref4">[4]</a>,<a href="#ref5">[5]</a>).
      </ul>
  <li> References<br>
<ol class="paren">
  <li><a name="ref1"></a>
      S.Kurogi:Asymptotic optimality of competitive associative nets for
	their learning in function approximation.
Proc. ICONIP2002, pp.507-511  (2007) <a href="https://doi.org/10.1002/scj.10538">https://doi.org/10.1002/scj.10538</a><br>
      (Detailed Japanese Journal: S.Kurogi: Asymptotic optimality of competitive associative nets and its Application to incremental learning of nonlinear functions,
	    Systems and Communications in Japan, Vol.J86-D-II, No.2, pp.184-194 (2003))<br><br>
  <li><a name="ref2"></a> S.Kurogi: Improving generalization performance via out-of-bag estimate using variable size of bags, J. Japanese Neural Network Society, Vol.J.16, No.2, pp.81-92 (2009) http://doi.org/10.3902/jnns.16.81<br>
      (An application written in English)
      S.Kurogi, R.Shigematsu, and K.Ono: Properties of Direct Multi-Step Ahead Prediction of Chaos Time Series and Out-of-Bag Estimate for Model selection 
      ICONIP2014, Part II, LNCS8835, pp. 421-428 (2014) <a href="https://doi.org/10.1007/978-3-319-12640-1_51">https://doi.org/10.1007/978-3-319-12640-1_51</a>
      <br><br>
<li><a name="ref3"></a> D.Miyazaki, K.Matsuo, and S.Kurogi: Entropy of LOO Predictable Horizons to Select a Learning Machine and a Representative
    Prediction of Chaotic Time Series, ICONIP 2020, CCIS 1333, pp. 778-787 (2020)
    <a href="https://doi.org/10.1007/978-3-030-63823-8_88">https://doi.org/10.1007/978-3-030-63823-8_88</a><br><br>
<li> <a name="ref4"></a>
    T.Tagomori, R.Tsuruda, K.Matsuo, S.Kurogi:
Speaker verification from mixture of speech and
non-speech audio signals via using pole distribution 
of piecewise linear predictive coding coefficients, 
    J Ambient Intell Human Comput (2020) <a href="https://doi.org/10.1007/s12652-020-01716-6">https://doi.org/10.1007/s12652-020-01716-6</a><br><br>
<li> <a name="ref5"></a>
    H.Nakayama, K.Ogi, K.Matsuo, and S.Kuro: 
Composition and Analysis of Pareto
Optimal Compromise Solutions
for Multiobjective Robust Controller
    Using GPC and CAN2s,
    ICONIP 2020, CCIS 1333, pp. 713-722, 2020.
<a href="https://doi.org/10.1007/978-3-030-63823-8_81">https://doi.org/10.1007/978-3-030-63823-8_81</a>
<li> <a name="ref6"></a> T.Tagomori, R.Tsuruda, K.Matsuo, S.Kurogi: Speaker verification from mixture of speech and non-speech audio signals via using pole distribution of piecewise linear predictive coding coefficients, J Ambient Intell Human Comput (2020) <a href="https://doi.org/10.1007/s12652-020-01716-6">https://doi.org/10.1007/s12652-020-01716-6</a>
<li> <a name="ref7"></a> H.Nakayama, K.Ogi, K.Matsuo, and S.Kuro: Composition and Analysis of Pareto Optimal Compromise Solutions for Multiobjective Robust Controller Using GPC and CAN2s, ICONIP 2020, CCIS 1333, pp. 713-722, 2020. <a href="https://doi.org/10.1007/978-3-030-63823-8_81">https://doi.org/10.1007/978-3-030-63823-8_81</a>
<li> <a name="ref8"></a> K.Taniguchi, S.Kurogi:Performance Improvement of a Method Using Entropy of LOO Predictable Horizons to Select a Representative Prediction of Chaotic Time Series, Proceedings of IEEE-CSDE 2022 (2022/12/19)
<li> <a name="ref9"></a> K.Komatsu, S.Oyabu, S.Kurogi: Performance Improvement and Analysis of a Method for Piano Authentication from Audio Signal by Combining Feature Vectors of LPC Spectral Envelope, MFCC, and pLPC Pole Distribution, 
Proceedings of IEEE-CSDE 2022 (2022/12/19) 
<li> <a name="ref10"></a> K.Komatsu, S.Kurogi: Performance Improvement for Speaker Detection from Mixed Speech of Multiple Speakers Using Probabilistic Prediction and Combining LPCSE, MFCC and pLPCPD, 
Proceedings of IEEE-CSDE 2022 (2022/12/19) 
    </ol>

    <hr>
<address></address>
<!-- hhmts start -->
Last modified: Mon Jun 12 15:19:51 JST 2023
<!-- hhmts end -->
</body> </html>
